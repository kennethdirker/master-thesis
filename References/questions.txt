LOFAR calibration:
    ? How does the pipeline work?
        ? Station specific or collectively?
            All readings are saved on disk in shared file storage.
            After this, all computing is done on the cluster (single node)!
            After each calculation step results are written to shared file storage!
            Shared storage consists of harddisk drives -> so a little slow! 
        - Green (Default PreProcessing Pipeline (DPPP):
            Predicting visibilities, manipulating the data, finding station-based solutions, applying these to the data
            ? What are visibilities?
        - Yellow (LOFAR Solution Tool (LoSoTo)):
            Isolate direction independant systematic effect.
            ? Deeper explanation?

    https://arxiv.org/pdf/1811.07954
    ? How much do we have to account for steps failing?
        ? Data from previous steps is available in shared filesystem. Restart step?
            ? If combined steps become too big, is the performance penalty
              small enough for the system to be justified?
                - Ask how many failures occur with Toil!
                    - We don't have to support execution fail saves
    
    ? Is Long Term Archive used for intermediate results?
        - Yes, right now results are written to the shared long term archive 
          after every step.

LINC:
    ? Are all steps pure python scripts?
        - No, javascript expressions are present in some steps.
        - No, command line programs are used.
        - No, inline python scripts are used.
        ? Why is steps/plot_unflagged.cwl not a python script file?
        ? Why do javascript scripts like check_demix.cwl exist?
            * I assume it's because it's faster and doesn't require 
            additional container setup?
            -I was right about this! The JS expressions restructure
            the data structures for the next workflow step.
                - These don't have to stay as JS! 


CWL:
    ? What is Schema Salad?
        https://www.commonwl.org/v1.2/SchemaSalad.html#Identifier_resolution
    Data Links:
        Connection from a "source" to a "sink" parameter.
        ? Used in this project?
            Referenced in the CWL Command Line Tool docs

DASK 
    ? How do we use containers with DASK?
        - DASK starts containers using singularity python bindings.

    ? What Task Scheduler to use?
        - Fully depends on the workload. Should ask for running statistics.
            * Problem for later.

Containers/Pods:
    - Use singularity to manage containers.
    ? How do we extract results from containers and write them to shared memory?
        - Mount a volume that acts as input/output gate.

Compiler:
    ? How to check input and output types?
    ? Convert JS to Python?
        - CWL-utils provides features to do this. Doable if enough time is left.
        ? Do we convert to a quicker compiled language?
            - No!
    ? How to run steps?
        - CWL step document needs to be converted to a python function, which
          can be used to build a DASK Task Graph.
    ? How to specify where a container is run?
        - Dask provides ways to name workers, which can be specified to run a
          command on when a task is submitted to the client.
          https://distributed.dask.org/en/latest/locality.html?highlight=client.submit
          ? Do we even want to do this?
            - Probably needed to keep track of where which resource is.
    ? How to check whether convertion was succesful?
        - Use Benten CWL visualizer and DASK build-in Task graph visualizer to
          manually compare graphs. 
        - Hard to do mathematically, skip!
    ? End solution applicable to all CWL?
        - Not all functionality is used in the pipeline, first work on useful features! 
        - CWL 1.2 is must have, could be expanded to work on newer versions.